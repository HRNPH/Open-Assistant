model_name: xlm-roberta-base
learning_rate: 3e-5
gradient_accumulation_steps: 16
per_device_train_batch_size: 2
max_length: 512
freeze_layer: 12
num_train_epochs: 2
datasets:
  - hfsummary